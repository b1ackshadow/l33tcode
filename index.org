#+title: DSA


* Arrays
** [[https://leetcode.com/problems/set-matrix-zeroes][Set matrix zeroes]]

- Brute force
  It will take O((N*M)*(N + M)) + O(N*M). No space

- Optimal 1
  We can use two arrays for row and col to mark indices which have 0. Final iteration to mark all cels whose index is on row or col array. This works but extra space. We need a way to use row and col
  without extra space

- Optimal 2
  We can use col0 for every row to mark row_map and row0 cols to map col with 0. but cell 0,0 will overlap. So use the 0th col completely for and col[1:] for cols with extra variable =col0= for 0th col. Now we have row and col markers without extra space.

  But we still need to make sure we dont directly overwrite the cell variables. First do 1,1 to M,N cells as they only rely on their respective row and cols no funny business.
  Then we know first row cols depend on the actual row[0] which itself depends on =col0= so if were to overwrite row0 then all the row0 cols would be 0. So first we update row0 cols[1:] then we update col[0] for all rows[1:] based on =col0= value.

** [[https://leetcode.com/problems/next-permutation][31 Next permutation]]
 This is an actual algorithm from 14th century so no need to sweat it. Just know this algorithm.

 [[https://en.wikipedia.org/wiki/Permutation#Generation_in_lexicographic_order][Original Algorithm]]

- Brute force
  a. generate all permutations
  b. sort it to find the next permuatation

For finding, all possible permutations, it is taking N!xN. N represents the number of elements present in the input array. Also for searching input arrays from all possible permutations will take N!. Therefore, it has a Time complexity of O(N!xN).

- Optimal
  We use some observations to develop our algorithm

  a. We need to find the largest prefix possible where there is a break point a[i] < a[i + 1]. Search from the end.
  b. Once we find the breakpoint, we know there are numbers which are greater than breakpoint which means we can try finding the next closest number. This means find the smallest element that is bigger than a[breakpoint]. Lexographically this number would come after the previous prefix of breakpoint. Find and replace the breakpoint number with this smallest number on the right that is bigger than a[breakpoint].
  c. Now after point b we have a prefix > original prefix. We need to make sure the resulting number is the closest which would mean the *next*. We can do this by sorting the right part.


  =Wikipedia version=,

  a. Find the largest index k such that a[k] < a[k + 1]. If no such index exists, the permutation is the last permutation.
  b. Find the largest index l greater than k such that a[k] < a[l].
  c. Swap the value of a[k] with that of a[l].
  d. Reverse the sequence from a[k + 1] up to and including the final element a[n].

* Graphs

*Foundations*

*** Traversal

a. BFS or DFS for basic traversal
b. Topological sorting using BFS or DFS for pre requisite type problems like course schedule
c. Kruskals MST algo for undirected weighted graphs using Disjoint set datastructure

** [[https://leetcode.com/problems/number-of-islands/][Number of islands]]
   Just use basic dfs and mark visited islands. For trick, we can just mark the grid[x][y] = 0 since we will only visit every node once hence =O(mn)= .
** [[https://leetcode.com/problems/max-area-of-island/][Max area of island]]
   Same idea as number of island. We need to track area for every node visited. if you a visit a node, then there will be atleast 1 area from this node. Based on each direction possiblity, they will return a local area i.e, area possible from that node (UP,DOWN,LEFT,RIGHT)

   so dfs will return 1 + local_area for every call

   Mistakes:
        Be careful when you check =valid_move()= . Always call it before exploring a particular move because when we generate the possible moves it may expire by the time recursion comes back.
** [[https://leetcode.com/problems/clone-graph/][Clone graph]]

Idea is pretty straight forward, we need to traverse all nodes starting from root node. clone the current node and link the neighbors. We link by maintaining node.val -> cloned_node ref.
If a neighbor does not exist in the hashmap, dfs on it so that the node gets created and is linked with its neighbors. so everytime we clone in the =dfs= we update it on hashmap so that any further
node which needs to link to this node can find the cloned ref and link it. when recursion comes back all the calling nodes can find the neighbor node and link up.

DFS, Hashmap

Edge cases:
- no node meaning return None
- at least 1 node but no neighbors
- any number of nodes and neighbors

  We are told that nodes start with 1 indexed. If nodes exists then root node would be 1. we return cloned_nodes.get(1, None) a ref to cloned root node
** [[https://leetcode.com/problems/rotting-oranges][Rotten oranges]]

Probably is an extension of number of island. First inistictive is to use DFS to find the time but the =EDGE= case is that there can be any number of rotten oranges at =time = 0=.

*Note*: Since need to process multiple oranges in every iteration by this we dont mean multiprocessing but model the problem so that every adjacent cell gets the parent cell' =time + 1=

Algorithm:
1. Initialize the queue with rotten oranges at time 0
2. start processing every cell, where we get the possible moves for this cell meaning that move[i] is  a valid cell and has a fresh orange.
3. for each of the adjacent cell, we have to use prev time + 1 as the processed time for this cells. So when they are processed from the queue(when we pop), we know the time after they were originally rotten.
4. Using its local time, we can keep tracking the new time for all the adjacent cells
5. This way all the same level(or t = time) oranges are processed in the same timeframe and consequently the resulting adjacent cells from these parent cells are also processed in the same time frame.
6. The last node processed will be the last processed orange meaning its time will be the total time. So we can keep updating this and return the last_time if there are no fresh oranges left.

Time complexity: BFS so =O(mn)=
Space complexity: same =O(mn)=

** [[https://leetcode.com/problems/pacific-atlantic-water-flow/description/][Pacific Atlantic Waterflow]]

Initial idea :-

for a given cell, we can do following operations :
1. check if it goes out of boundary depending on the ocean we are trying to reach
2. if it does check for each possible move whether we can go based on height values if not skip it
3. If we can reach an adjacent cell, check if its already visited?
4. If its is visited, we can check the status of this cell whether it was able to reach the ocean
5. update the status for the calling original cell.

We can maintain visited map, if a cell was visited it would contain the status of reaching the ocean for example, visited[(0,0)] = (True, False)

*Better idea*

- Treat the problem as flooding based on the condition that flood can go to x_n, y_n if height_prev <= height_n.
- Initially we can mark the coasts as a starting point for flood
- Iterate for every cell, if the cell is flooded we can explore
- For all possible valid moves, we flood it and then try going further from that cell
- We can explore both oceans using initial coast cells.

Optimizations:

- We can only start the flood from coasts so we need two loops

  1. first and last column for all rows for pacific and atlantic respectively.
  2. first and last row for all columns for pacific and atlantic respectively.

Note: Also think of BFS solution. We have implemented and logic remains pretty much same just use queue as task manager. if an adjacent cell is reachable add it to queue. everytime we pop a cell we mark it as rechable in the set and return the set from bfs. we can construct both queue get both reacables and P & Q set operation.


#+begin_src python
def dfs(self, x, y, ocean):

moves = self.get_moves(x, y)

for move in moves:
    x_n, y_n = move
        if (
                self.is_valid(x_n, y_n)
                and self.heights[x][y] <= self.heights[x_n][y_n]
                and (move not in ocean)
        ):
            ocean.add(move)
            self.dfs(*move, ocean)
#+end_src

** [[https://leetcode.com/problems/surrounded-regions][Surrounded regions]]

Think of the flooding method from pacific ocean problem. We use a similar approach. Use bfs as it is more convenient and avoid recursion which ends up missing up the logic.

- We know the "O" on borders are "reachable" meaning they are not surrounded.
- Any intermediate "O" has to find a border "O" to be alive if not it gets turned to "X"
- This means we only have to start BFS from the borders - Fill up the queue with border "O"s and start exploring adjacent "O"s that are not already reachable(this also tells that an O is visited or not). When we find a new O mark it as reachable
- Iterate through the board and turn all the Os which are not reachable to "X"

** [[https://leetcode.com/problems/course-schedule/][Course schedule]]
So the problem statement is can we essentially visit all nodes and process them given that we have visited the indegree nodes or depedency courses in this case. Based on this idea, we have to use topological sort which uses indegrees to find and process a node with 0 indegree value bcuz we can only do a course if it has 0 depedency at the time of processing.

Steps:-
1. First build the adjacency graph
2. Compute the indegrees array for all the nodes using the prerequisites depedency array
3. To start with we need a 0 dependency node. If we cant find then obviously its a cycle we cant process return =False=.
4. Queue processing starts with 0 node if possible, then process the node -mark it as -1- so that we dont revisit it. decrement all the adjacent nodes.
5. Find and update queue with a new 0 node.
6. Return =processed= == =numCourses= .

** [[https://leetcode.com/problems/course-schedule-ii/][Course schedule II]]
Same algorithm as part 1. We just need to track the processed nodes for the order. everything else pretty much remains same.
** [[https://leetcode.com/problems/course-schedule-iii/][Course schedule III]]
This is a little different actually. Nothing to do with Graphs really.

Observations:

- Firstly, we are trying to maximize the number of courses completed
- If we want to complete max courses, we need to start finishing courses that are sorted by their deadline
- We sort the courses, start processing each course as long it does not exceed its deadline
- We need to track total time so far for the courses we have completed
- if adding a new course does not exceed its deadline keep going
- If it exceeds the deadline, we need to remove the longest duration course this way we know the number of courses completed remain same except we will be reducing the total_time_taken
- If we can reduce total time taken, we can probably complete another course
